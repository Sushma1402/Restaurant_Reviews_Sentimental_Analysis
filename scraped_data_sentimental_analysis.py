# -*- coding: utf-8 -*-
"""Scraped_Data_Sentimental_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dqKMAywowGBIHcbQE9GmzeFGKB7P8pJ9
"""

# Natural Language Processing(NLP) - zomato Reviews
# In this project we are going to perform Natural Language Processing. We are going to use CountVectorizer( Bag Of Words) Technique to predict Positive and Negative reviews.

# Problem Statement:
# To verify the Sentiment Analysis to determine whether the data is positive or negative


# General Packages
import pandas as pd
import csv
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
warnings.filterwarnings('ignore')

# NLP packages
# Import the nltk function
import nltk
# from nltk library import the stopwords
from nltk.corpus import stopwords
# Import the string
import string
# From this feature_extraction import Countvectorizer
# Countvectorizer will count that the particular word appears for how many times 
from sklearn.feature_extraction.text import CountVectorizer

#to store the new CSV file
OUTPUT_PATH = '/content/'
# Read the file
# zomato = "/Users/sushmapawar/Desktop/Projects/SentimentalAnalysis/sentiment_analysis_output"
zomato = pd.read_csv(r'/content/sentiment_analysis_output.csv')

# to know how many rows and columns dataframe has
print('DATAFRAME contains how many rows and columns:\n',zomato.shape)

# to know the datatypes
print('DATATYPES:\n',zomato.info())
# print(zomato.describe().transpose())


# Check the head function
print('FIRST ROWS OF DATAFRAME ARE:\n',zomato.head())


# # Check the nulls
print('How many NULLS are their in zomato Dataframe:\n',zomato.isnull().sum())

#--------------DATA PREPARATION--------------------------------

# replacing blanks in Written_Review with "-"
zomato["Written_Review"].fillna("-",inplace=True)
print(zomato)

ax = zomato['User_Rated_Restaurant'].value_counts().sort_index().\
      plot(kind ='bar',title = 'Count of Reviews by stars',figsize=(10,5))
ax.set_xlabel('User_Rated_Review_Stars')
plt.show()

# Replace the target column User_Rated_Restaurant with 0 and 1
# If the user rating is greater than 3 replace it with 1 &
# If the user rating is lesser than and equal to 3 replace it with zero
# zomato['User_Rated_Restaurant'] = np.where(zomato['User_Rated_Restaurant'] <= 3 , 0, zomato['User_Rated_Restaurant'])
# zomato['User_Rated_Restaurant'] = np.where(zomato['User_Rated_Restaurant'] > 3 , 1, zomato['User_Rated_Restaurant'])

# Mapping the ratings
zomato['Sentiment_Rating']= np.where(zomato.User_Rated_Restaurant >3,1,0)
# removing neutral reviews
zomato = zomato[zomato.User_Rated_Restaurant !=3]
# printing the counts of each class
zomato['Sentiment_Rating'].value_counts()
zomato.Sentiment_Rating.value_counts().plot.bar()
plt.show()

print(zomato)
print(zomato.shape)
# print('sampleper class: {}'.format(np.bincount(zomato.User_Rated_Restaurant)))

# to now the 0's and 1's of target column
print('How many zeros and onces are present in target column:\n',zomato.Sentiment_Rating.value_counts())

# Remove not required columns
zomato = zomato.drop(['Unnamed: 0'],axis=1)
zomato = zomato.drop(['Restaurant_Name', 'Restaurant_rated','Review_Written_By'], axis=1)

# dowload the Data cleaned CSV
zomato.to_csv(OUTPUT_PATH + 'Cleaned_Dataset_Zomato.csv')

# After performing EDA how many rows and columns dataframe has
print('After EDA dataframe has rows and columns:',zomato.shape)

#-----------------------SENTIMENTAL ANALYSIS---------------------------
# Convert all the data into lower case of english alphabate
# As python is case sensitive, we are converting all those data into lower case
# Similarly, we can also convert it into Upper case also.
zomato.Written_Review = zomato.Written_Review.str.lower()

# count the length of Written review for each row and add that column in dataframe zomato
zomato["Length"] = zomato.Written_Review.apply(len)
# to check length is counted and length column is added in dataframe
print(zomato.head())
print(nltk.download('stopwords'))  #True download stopword

# Check for all the stopwords from enlgish directory
print(stopwords.words("english"))

# Length of stopwrods from english directory
len(stopwords.words("english"))

# Check the punctuations from the strings
print(string.punctuation)

# Define a function to remove the punctuation and stopwords by selecting those them specially. 
# Where text_process is the use defined function 

def text_process(mess):
    """## a docstring
    1.remove the stopwords
    2.remove the punctuation
    3.return the list of clean textwords
    
    """
    nonpunc = [char for char in mess if char not in string.punctuation]
    nonpunc = "".join(nonpunc)
    
    return[ word for word in nonpunc.split() if word not in stopwords.words("english")]

# Above is the user defined function which will remove the punctuations and stopwords from the data. We will apply this function and get our data cleaned

# In this step we are applying the text_process user defined function on the 'Written_Review' column 
# And having the data which contains the words from corpus except stopwords and punctuation 
print(zomato['Written_Review'].apply(text_process))

# What is the maximum and minimum lengths of the message? and which are those message?
print(zomato["Length"].max())
print(zomato[zomato["Length"] == zomato["Length"].max()]["Written_Review"].iloc[0])
print(zomato["Length"].min())
print(zomato[zomato["Length"] == zomato["Length"].min()]["Written_Review"].iloc[0])
print(sns.barplot(x = "Sentiment_Rating", y = "Length", data = zomato))
# to know length of words for 1 is more or for zero
print(zomato.hist(column= "Length",by="Sentiment_Rating", bins = 50))

# We are counting words form text_process user defined fucntion from Written_Review columns 
bow_transfer = CountVectorizer(analyzer= text_process).fit(zomato["Written_Review"])
print('Words Counted:',bow_transfer)

# Above function we are checking the vocabulary by counting that how many times it appears
print(bow_transfer.vocabulary_)

# Length of the vocabulary
print(len(bow_transfer.vocabulary_))

# We are just transforming the bow_tranfer into zomato_Written review by using transform
# Convert the TDM (term document matrix)
review_bow = bow_transfer.transform(zomato.Written_Review)
print(review_bow)


# Check the shape function 
print(review_bow.shape)

# As we run the type fucntion over it, it will execute the sparse.matrix
# Sparse. matrix = (which contains only few number  no numierical data)
print(type(review_bow))

#  SAMPLING

# Split the term in train and test
from sklearn.model_selection import train_test_split
x_train , x_test, y_train, y_test = train_test_split(review_bow , zomato.Sentiment_Rating, test_size = 0.2 , random_state = 123)

# Input the NB algorithm
# We can also use DT , RF , Logistic Regression, etc. 
from sklearn.naive_bayes import MultinomialNB
NB = MultinomialNB()
print(NB)

# Fit the model over train data 
NB.fit(x_train, y_train)

# PREDICTION
pred_zomato = NB.predict(x_test)
print('Prediction:\n',pred_zomato)

print('Actual:\n',y_test)

#Import the confusion matrix from metrics
from sklearn.metrics import confusion_matrix

# Check the confusion matrix 
tab_zomato = confusion_matrix(pred_zomato, y_test)
print('CONFUSION MATRIX IS:\n',tab_zomato)

# Check the accuracy for the formed model 
Acc = tab_zomato.diagonal().sum() / tab_zomato.sum() * 100
print('OUR MODEL HAS ACCURACY OF:',Acc ,'%')



"""# New section"""